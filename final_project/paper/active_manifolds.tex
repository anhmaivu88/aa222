\documentclass[]{aiaa-tc}% insert '[draft]' option to show overfull boxes

\input{../zachs_macros}

 \usepackage{varioref}%  smart page, figure, table, and equation referencing
 \usepackage{wrapfig}%   wrap figures/tables in text (i.e., Di Vinci style)
 \usepackage{threeparttable}% tables with footnotes
 \usepackage{dcolumn}%   decimal-aligned tabular math columns
  \newcolumntype{d}{D{.}{.}{-1}}
 \usepackage{nomencl}%   nomenclature generation via makeindex
  \makenomenclature
 \usepackage{subfigure}% subcaptions for subfigures
 \usepackage{subfigmat}% matrices of similar subfigures, aka small mulitples
 \usepackage{fancyvrb}%  extended verbatim environments
  \fvset{fontsize=\footnotesize,xleftmargin=2em}
 \usepackage{lettrine}%  dropped capital letter at beginning of paragraph
 % \usepackage[dvips]{dropping}% alternative dropped capital package
 \usepackage[colorlinks]{hyperref}%  hyperlinks [must be loaded after dropping]
 \usepackage{booktabs} % for \toprule and \bottomrule

 \title{Pursuing Active Manifolds via Nonlinear Optimization}

 \author{
  Zachary R. del Rosario\thanks{PhD Candidate, Aeronautics and Astronautics, 496 Lomita Mall, Stanford CA, AIAA Student Member.}\\
  {\normalsize\itshape
   Stanford University, Stanford CA, 94305, USA}\\
 }

 % Data used by 'handcarry' option
 % \AIAApapernumber{YEAR-NUMBER}
 % \AIAAconference{Conference Name, Date, and Location}
 % \AIAAcopyright{\AIAAcopyrightD{YEAR}}

 % Define commands to assure consistent treatment throughout document
 \newcommand{\eqnref}[1]{(\ref{#1})}
 \newcommand{\class}[1]{\texttt{#1}}
 \newcommand{\package}[1]{\texttt{#1}}
 \newcommand{\file}[1]{\texttt{#1}}
 \newcommand{\BibTeX}{\textsc{Bib}\TeX}

 % Define document-specific shortcuts
 \newcommand{\dom}{\mathcal{D}}

\begin{document}

\maketitle

\begin{abstract}
Dimension Reduction is an approach to alleviating the Curse of Dimensionality. Active Subspaces is a dimension reduction technique which seeks linear subspaces which are `active'; that is, they capture the majority of variation in some quantity of interest (QoI). While useful in practice, Active Subspaces do not always exist. A natural generalization is to seek \emph{Active Manifolds}, curved spaces which capture variations in our QoI. The problem of finding Active Manifolds is posed here as a nonlinear optimization problem, and solved using interior point methods.
\end{abstract}

\printnomenclature % creates nomenclature section produced by MakeIndex

%-------------------------------------------------
\section{Introduction} \label{sec:introduction}
%-------------------------------------------------

\lettrine[nindent=0pt]{O}{ne} of the most challenging difficulties facing high-fidelity modeling is the treatment of high-dimensional parameter spaces: the Curse of Dimensionality. Consider a parameter study on some quantity of interest (QoI) $f$ in a space $\dom\subseteq\mathbb{R}^m$; a simple heuristic is to use $10$ points per dimension, in order to well represent the parameter space. Then the total number of sample points is $10^m$. If a computer code implementing our model executes in a fixed time of $1$ second, then our parameter study execution time scales exponentially. Figure \ref{fig:curse_of_dimensionality} depicts the aforementioned scenario.
\nomenclature{$\dom$}{Domain of $f$}%

The only reasonable strategy to mitigate this challenge is to perform \emph{dimension reduction}; that is, to reduce $m$. One scheme for dimension reduction of this sort is to seek \emph{Active Subspaces} -- linear subspaces in parameter space along which the majority of variation in our QoI is captured. \cite{constantine2015} The Active Subspace approach gives a `perfect' dimension reduction in the case that our QoI is a Ridge Function; that is, for $\vx\in\mathbb{R}^m$ and $\mA\in\mathbb{R}^{m\times k}$ with $k<m$, we have $f(\vx)=g(\mA^T\vx)$. Note that a Ridge Function is constant along directions which are orthogonal to $\mA$, that is

\begin{equation}
\mW^T\nabla f = 0 \logeq \mW^TA = 0, \label{eq:ridge_property}
\end{equation}

where $\mW$ is an $m\times d$ matrix, and $\nabla f$ is the gradient of $f$, understood to be a column vector. One example of a Ridge Function is $f(\vx) = \frac{1}{2}(.7x_1+.3x_2)^2$. In this case, the Active Subspace approach discovers the Active and Inactive directions, depicted in Figure \ref{fig:as_example}. Note that this gives us a `perfect' dimension reduction, as we can completely neglect changes along the direction $[-.3,.7]^T$. Such Ridge Functions may seem like a contrivance, but they are actually quite common in multivariate Fourier Transforms\cite{pinkus2015} and physical laws in general.\cite{Constantine2016} Nevertheless, Ridge Functions are not the only functional form that arises in practice. In these cases the Active Subspace is approximate. While approximate Active Subspaces are useful in applications such as aerodynamic shape optimization\cite{lukaczyk2014}, scramjet analysis\cite{Constantine20151}, and hydrologic modeling\cite{Jefferson2015}, it is easy to construct functions which do not admit this sort of low-dimensional structure, even in an approximate sense. In these cases, we would like to do more than simply give up.

\begin{wrapfigure}{R}{0.5\linewidth}
 \includegraphics{../images/curse_of_dimensionality}
 \caption{Execution time scales exponentially with the dimension of parameter space.}
 \label{fig:curse_of_dimensionality}
\end{wrapfigure}

A natural generalization of Active Subspaces is to seek \emph{Active Manifolds}; that is, curved subsets of parameter space which capture the variability of a function. Such low-dimensional structures should recover linear subspaces in the case that they exist (i.e. a Ridge Function), and more general spaces when they do not. Note that if a function is differentiable, we can always move along the gradient to capture the full variability of a function -- unfortunately this requires perfect knowledge of the function, which brings us back to the Curse of Dimensionality. In practice we must use a limited number of gradient samples (assumed to be available, say through an adjoint solution\cite{Jameson1988} or automatic differentiation\cite{Rall1981}) to numerically approximate such a manifold.

\begin{wrapfigure}{R}{1.0\linewidth}
 \includegraphics{../images/surface_plot}
 \caption{Note that on average, the function changes more along the active directions than the inactive directions; in this case, the change in the inactive direction is exactly zero.}
 \label{fig:as_example}
\end{wrapfigure}

In this work, we focus on the \emph{identification} of Active Manifolds, and leave their usage to subsequent works. The re-parameterization of a function on an Active Subspace is already laden with important considerations, which is further complicated by generalization to more arbitrary spaces. These are important issues which lie outside the scope of this document.

%-------------------------------------------------
\section{Seeking Active Manifolds} \label{sec:active_manifolds}
%-------------------------------------------------
The strategy we will adopt in this work is to generalize the properties of a Ridge Function: By allowing Equation \ref{eq:ridge_property} to vary in space, we arrive at

\begin{equation}
  \label{eq:inactive_manifold}
  \mW(\vx)^T \nabla f(\vx) = 0.
\end{equation}%
\nomenclature{$\vx$}{Input vector}%
\nomenclature{$f(\vx)$}{Scalar quantity of interest}%
\nomenclature{$\nabla f(\vx)$}{Gradient of QoI}%
\nomenclature{$\mW(\vx)$}{Manifold matrix}%

Equation \ref{eq:inactive_manifold} gives us a set of directions $\mW(\vx)$ along which $f(\vx)$ does not vary -- these directions define \emph{Inactive Manifolds}, while the orthogonal directions define \emph{Active Manifolds}. Note that as long as $f(\vx)$ is differentiable we know $\mW(\vx)$ exists, as we can simply perform a QR decomposition on the gradient $\nabla f$ augmented with the $m\times m$ identity matrix. While this is mathematically possible, such a scheme is computationally intractable, as it is an infinite dimensional problem. Instead, we must settle on a finite dimensional problem by approximating Equation \ref{eq:inactive_manifold}.

First, we will consider a single direction $\vw_j(\vx)$, and parameterize on a finite number of basis functions. Choose a set of $k$ differentiable functions $\phi_i:\mathbb{R}^m\to\mathbb{R}$, and set

\begin{equation}
\label{eq:parameterized_w}
\vw_j(\vx) = \sum_{i=1}^k \alpha_{ij}\nabla\phi_i(\vx),
\end{equation}
\nomenclature{$\vw_j(\vx)$}{Manifold vector}%
\nomenclature{$\vsym{\alpha}_j$}{Manifold parameter vector}%
\nomenclature{$\phi_i$}{Manifold basis function}%
\nomenclature{$k$}{Number of basis functions}%

where the $\alpha_{ij}$ parameterize $\vw_j(\vx)$ in a linear fashion. Now we construct $\mW(\vx)$ from a collection of these $\vw_j(\vx)$, each parameterized on a different $\vsym{\alpha}_j$ vector, that is

\begin{equation}
\mW(\vx) = [\vw_1(\vx),\dots,\vw_l(\vx)].
\end{equation}

Rather than attempt to solve for $\mW$ all at once, we will attempt to enforce Equation \ref{eq:inactive_manifold} for each $\vw_j(\vx)$ individually. We must make two concessions though. First, even though the $\vw_j$ are now parameterized on a finite set, Equation \ref{eq:inactive_manifold} is enforced at all points $\vx\in\dom$: In practice we must sample a number of points $\{\vx_i\}_{i=1}^n\subseteq\dom$ and enforce $\vw_j^T(\vx_i)\nabla f(\vx_i)=0$ on this set. Second, unless $\mW(\vx)$ lies precisely within the span of the $\nabla\phi_i$, Equation \ref{eq:inactive_manifold} will hold only approximately: In practice we will attempt to minimize the residual, defined by
\nomenclature{$n$}{Number of sample points in $\dom$}%

\begin{equation}
\label{eq:residual}
R = \|\mM\vsym{\alpha}\|_2,
\end{equation}

where

\begin{equation}
\label{eq:matrix_m}
\mM = \left[\begin{array}{ccc}%
    \nabla\phi_1^T(\vx_1)\nabla f(\vx_1) & \cdots & \nabla\phi_k^T(\vx_1)\nabla f(\vx_1) \\
    \vdots & \ddots & \vdots \\
    \nabla\phi_1^T(\vx_n)\nabla f(\vx_n) & \cdots & \nabla\phi_k^T(\vx_n)\nabla f(\vx_n) \\
    \end{array}\right].
\end{equation}

Note that if $\mM\vsym{\alpha}=0$, then Equation \ref{eq:inactive_manifold} holds exactly for $\mW=[\vw]$, parameterized on $\vsym{\alpha}$. Also note that this fixed $\mM$ defines the residual for each $\vw_j$, as they are each parameterized on the same basis, albeit with a different $\vsym{\alpha}$. Na{\"i}vely, we may seek to minimize the residual $R$ directly; however, this optimization problem has a trivial solution $\vsym{\alpha}=0$. In practice we constrain the length via $\|\vsym{\alpha}\|_2\geq1$. Additionally, note that we wish to solve for a \emph{collection} of $\vsym{\alpha}_j$; we can accomplish this by demanding that each new $\vsym{\alpha}_j$ be orthogonal to the previous ones. Finally, we add a 1-norm term to encourage sparsity; we would like each $\vsym{\alpha}_j$ to be `simple', in the sense of being sparse. This leads to the following sequence of $d$ optimization problems
\nomenclature{$d$}{Number of manifolds sought}%

\begin{equation}
\begin{aligned}
\label{eq:optimization}
\text{min  }& \|\mM\vsym{\alpha}_j\|_2 + \beta\|\vsym{\alpha}_j\|_1, \\
\text{s.t. }& \|\vsym{\alpha}_j\|_2\geq1, \\
            & \mA_j^T \vsym{\alpha}_j = 0,
\end{aligned}
\end{equation}

for $j=1,\dots,d$, where $\beta>0$ is a tunable weighting parameter (in practice $\beta=1$), and $\mA_j=[\vsym{\alpha}_1,\dots,\vsym{\alpha}_{j-1}]$, with $\mA_1=0$. There are a few important points to note: First, one should set $d$ equal to the number of Inactive Manifolds sought -- one could either define a fixed number of manifolds to seek, or continue running until the residual cannot be reduced under some desired tolerance. The former method is appropriate if the degree of dimension reduction required is already known, while the latter will discover the maximum reduction available, subject to the choice of basis. Second, Problem \ref{eq:optimization} has a convex objective function but a nonlinear constraint, thus we do \emph{not} enjoy the benefits of a convex problem. This creates issues when attempting to design a solver, which will be addressed in Section \ref{sec:solver}:\ref{sec:solver_design}.

%------------------------
\subsection{Choice of Basis} \label{sec:basis}
The choice of basis functions $\phi_i$ is crucial for obtaining an Active Manifold. The basis must be chosen to include $\mW(\vx)$, at least in some approximate sense. In the case of a Ridge Function the Active Manifolds are linear subspaces, so a linear choice of basis $\phi_i(\vx) = x_i$ is appropriate. In fact, choosing a linear basis and approximating Equation \ref{eq:inactive_manifold} by finding the nullspace via a Singular Value Decomposition (SVD) exactly recovers the Active Subspace procedure! To see this, substitute the linear basis into Equation \ref{eq:matrix_m} to find

\begin{equation}
\label{eq:matrix_m_linear}
\mM_l = [\nabla f(\vx_1),\dots,\nabla f(\vx_n)]^T.
\end{equation}

Equation \ref{eq:inactive_manifold} corresponds to the condition $\mM_l\vsym{\alpha}=0$, which is a nullspace computation. This can be accomplished by considering the SVD of $\mM_l=U\Sigma V^T$; the vectors $\vv_i$ which correspond to the zero singular values $\sigma_i$ form a basis for the nullspace of $\mM_l$. These are found via an eigenvalue decomposition of $\mM_l^T\mM_l$, which equals

\begin{equation}
\label{eq:sum_m}
\mM_l^T\mM_l = \sum_{i=1}^n \nabla f(x_i) \nabla f^T(x_i).
\end{equation}

Note that the Active Subspace is also found via an eigenvalue decomposition of the $\mC$ matrix, defined as the weighted average of the outer product of the gradient.\cite{constantine2015} Compare Equation \ref{eq:sum_m} to the Monte Carlo approximation of the $\mC$ matrix

\begin{equation}
\label{eq:sum_c}
\hat{\mC} = \frac{1}{n} \sum_{i=1}^n \nabla f(x_i) \nabla f^T(x_i).
\end{equation}

Note that up to a factor of $n$, they are the same! Thus, for the correct choice of basis, the Active Manifold procedure defined above should recover Active Subspaces. This is important, as it shows that if our basis is chosen correctly and Problem \ref{eq:optimization} approximates Equation \ref{eq:inactive_manifold} well, we can do no worse than Active Subspaces.

In this work, we try a number of different sets of basis functions, defined in Table \ref{tab:basis} below. These include both the linear basis and larger sets.

\begin{table}
\centering
\begin{tabular}{ll}
\toprule
Name & Basis \\
\midrule
Linear & $x_i$ \\
Quadratic & $x_i$, $x_i^2$, $\log(|x_i|)$ \\
\bottomrule
\end{tabular}
\caption{Basis sets used in numerical experiments. Note that each basis function is used in each dimension; thus for $m=3$, the Linear set has $3$ basis functions, while the Quadratic set has $9$.}
\label{tab:basis}
\end{table}

%-------------------------------------------------
\section{Solver} \label{sec:solver}
%-------------------------------------------------
In order to solve Problem \ref{eq:optimization}, a nonlinear solver was designed, implemented, and tested on a number of different quantities of interest $f$ and sets of basis functions $\phi_i$.

%------------------------
\subsection{Solver Design} \label{sec:solver_design}
As mentioned in Section \ref{sec:active_manifolds}, the optimization problem in question is a nonlinear optimization problem with nonlinear constraints. The solver is an implementation of the interior point method using log-barrier functions and a quasi-Newton method using the BFGS update rule.\cite{Chandrupatla1999} In this scheme, we first solve the feasibility problem by constructing an objective function on the constraints, then run the interior point method successively relaxing the barriers.

Note that one could construct a log-barrier on the linear constraint $\mA^T_j\vsym{\alpha}_j=0$. Instead, we employ \emph{progressive reparameterization}; that is, reparameterizing the problem such that the constraint is automatically satisfied. We accomplish this for step $j$ by constructing a set of vectors $[\vtq_{1j},\dots,\vtq_{k-j+1}]=\tilde{\mQ}_j$ orthogonal to all previous solutions $[\vsym{\alpha}_1,\dots,\vsym{\alpha}_{j-1}]=\mA_j$, and defining a new set of $k-j+1$ variables $\vtlm{\alpha}_j$ which replace the old $\vsym{\alpha}_j=\tilde{\mQ}_j\vtlm{\alpha}_j$. Note that by this construction $\vsym{\alpha}_j$ is orthogonal to all previous solutions.

Note that the first iteration $j=1$ of Problem \ref{eq:optimization} has no linear constraint, so the solver described above is sufficient. For subsequent iterations, we redefine the variables as described above and optimize over $\vtlm{\alpha}_j$. Our new optimization problem is

\begin{equation}
\begin{aligned}
\label{eq:optimization_reparam}
\text{min  }& \|\mM\tilde{\mQ}_j\vtlm{\alpha}_j\|_2 + \beta\|\tilde{\mQ}_j\vtlm{\alpha}_j\|_1, \\
\text{s.t. }& \|\tilde{\mQ}_j\vtlm{\alpha}_j\|_2\geq1,
\end{aligned}
\end{equation}

where $\vtlm{\alpha}\in\mathbb{R}^{k-j+1}$, and $\tilde{\mQ}_j$ is found by taking the last $k-j+1$ columns of the orthogonal matrix found from an economy QR decomposition of $\mA_j$, augmeted with the $k\times k$ identity matrix

\begin{equation}
\label{eq:augmeted_matrix}
[\mA_j,\mI] = \mQ\mR.
\end{equation}

Through this scheme, subsequent vectors are guaranteed to be orthogonal to numerical precision, contingent on the performance of QR. However, note that since the QR decomposition has a complexity of $O(n^3)$, this method is inappropriate for excessively high-dimensional spaces. In these cases, it may be prudent to approximate the linear constraint with a barrier function. However, it is worth noting that since the computational cost we are trying to alleviate is exponential, a cubic cost may still be comparatively cheap.

Finally, we must address the lack of convexity in our optimization problem. Problem \ref{eq:optimization} is non-convex, and thus may contain local minima. This is problematic with our interior point method, which for a poor choice of initial guess may converge to such a local optimum. To alleviate this issue, we employ a restart heuristic: We begin with a uniform random guess within a unit box in the positive orthant, and if at any stage $j$ the residual (Eq. \ref{eq:residual}) does not drop below an absolute threshold, we restart that stage with a new initial guess. This is an admittedly simple solution, which is not guaranteed to solve the problem, but it works well enough in practice.

%------------------------
\subsection{Solver Performance} \label{sec:solver_performance}
The solver described above was evaluated on a number of different test cases in order to determine its efficacy. These test cases are summarized in Table \ref{tab:tests}. Note that a Ridge Function in this case is a function of the form 

\begin{equation}
\label{eq:ridge_function}
f(\vx)=(\va^T\vx)^2, 
\end{equation}

where $\va$ is a random vector of unit length. A `Mixed Function' is a function of the form 

\begin{equation}
\label{eq:mixed_function}
f(\vx)=(a_1x_1+\dots+a_{\floor*{m/2}}x_{\floor*{m/2}}+a_{\floor*{m/2}+1}x_{\floor*{m/2}+1}^2+\dots+a_mx_m^2),
\end{equation}

where as before $\va$ is a random vector. Thus Equation \ref{eq:mixed_function} is `mixed' in the sense that it has multiple types of terms. Note that the Linear basis can recover the Inactive Subspace of a Ridge Function exactly, while the Quadratic basis can exactly recover the Inactive Manifolds of a Mixed Function. This was intentional; we leave the study of approximate Active Manifolds to a future work. 

Additionally, for each test case we fix the dimension of the objective function $m$, and the number of Inactive Manifolds sought $d$. In each case $d$ was chosen to maximize the dimension reduction. A residual threshold of $R_t=10^{-4}$ was set for the reset criteria, and $8$ resets per stage were allowed. After the number of resets is exceeded, the best run of the stage is taken, and the stage advanced. The $\vx_i$ are sampled uniform randomly from the unit hypercube on $\mathbb{R}^m$, and in each case a fixed number of $200$ samples are taken.

\begin{table}
\centering
\begin{tabular}{llll}
\toprule
Test Case & Quantity of Interest & Dimension & $d$\\
\midrule
$1$ & Ridge Function & $m=3$ & 2 \\
$2$ & Ridge Function & $m=10$ & 9 \\
$3$ & Mixed Function & $m=3$ & 2 \\
$4$ & Mixed Function & $m=10$ & 9 \\
\bottomrule
\end{tabular}
\caption{Test cases used to evaluate solver.}
\label{tab:tests}
\end{table}

As mentioned in Section \ref{sec:active_manifolds}:\ref{sec:basis}, the choice of a linear basis recovers Active Subspaces, thus our first test was to determine if the solver could recover this behavior. Figures \ref{fig:residual_case1} and \ref{fig:objective_case1} show the results of studying Test Case 1: A three-dimensional ($m=3$) Ridge Function with a one-dimensional Active Subspace.

\begin{figure}[!ht]
\centering
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/residual_case1}
 \caption{Typical residual sequence for Test Case 1.}
 \label{fig:residual_case1}
\end{minipage}%
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/objective_case1}
 \caption{Typical objective sequence for Test Case 1.}
 \label{fig:objective_case1}
\end{minipage}
\end{figure}

Figures \ref{fig:residual_case1} and \ref{fig:objective_case1} are typical results when studying such a Ridge Function; we see the rapid convergence rate of quasi-Newton's method in both convergence metrics. Since the residual measures the degree to which Equation \ref{eq:inactive_manifold} holds and $R$ is reaching machine zero for this case, this provides strong evidence that the solver is finding the correct Inactive Subspace. We can further verify this result by checking the subspace distance between the subspace found via our solver, and the Inactive Subspace found via the usual procedure (definition provided in the Appendix). In the case depicted above, the subspace distance is $\text{Dist}(\mW_{\text{solver}},\mW_{\text{AS}})=1.0786\times10^{-9}$; clearly, the two subspaces are the same to working precision.

Since our solver uses a random initial guess, the solver performance is a random variable. Some runs of the solver give abberant behavior, depicted below. Note that the convergence sequences depicted in Figures \ref{fig:residual_linear_long} and \ref{fig:objective_linear_long} do not qualitatively match; this demonstrates the importance of checking both the residual and objective values. Since the objective is a mixture of both the residual and log-barriers, it is not representative of the true quantity we are attempting to minimize.

\begin{figure}[!ht]
\centering
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/residual_linear_long}
 \caption{Abberant residual sequence for Test Case 1.}
 \label{fig:residual_linear_long}
\end{minipage}%
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/objective_linear_long}
 \caption{Abberant objective sequence for Test Case 1.}
 \label{fig:objective_linear_long}
\end{minipage}
\end{figure}

One final remark is that the number of iterations required by the solver increases with the dimensionality of both the problem and the basis. This dependence is shown in Figures \ref{fig:count_linear_basis_ridge_objective} and \ref{fig:count_quadratic_basis_mixed_objective}. These figures are produced by running test cases identical to those noted in Table \ref{tab:tests}, except for varying $m$. Note that the computational growth rate is slowing with increasing dimension, which is an attractive feature of the process. Note also that for each of these test cases, a \emph{fixed} number of samples $\vx_i$ are evaluated on $f$; thus the figures imply that we can get away with a relatively unchanged number of expensive samples, so long as we work harder on the optimization problem. This is precisely what we would like to see from a dimension reduction technique -- if we can offload evaluations of $f$ to some other process, then we can significantly accelerate our computational studies. The precise dependence on the number of samples $\vx_i$ is critical, and a definite candidate for future work.

\begin{figure}[!ht]
\centering
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/count_linear_basis_ridge_objective}
 \caption{Average iteration count (over $10$ runs) for a Ridge Function objective with a linear basis.}
 \label{fig:count_linear_basis_ridge_objective}
\end{minipage}%
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/count_quadratic_basis_mixed_objective}
 \caption{Average iteration count (over $10$ runs) for a Mixed Function objective with a quadratic basis.}
 \label{fig:count_quadratic_basis_mixed_objective}
\end{minipage}
\end{figure}

\newpage
The remaining convergence plots are qualitatively similar, and are thus relegated to the Appendix.

%-------------------------------------------------
\section{Conclusion and Future Work} \label{sec:conclusion}
%-------------------------------------------------

In this work, the problem of finding Active Manifolds was posed as a nonlinear optimization problem of finding Inactive Manifolds, and a solver was designed and tested to solve this problem. The solver was tested on a number of different cases, and favorable results were shown. Our results show that seeking Active Manifolds in this fashion is a potentially fruitful line of inquiry. However, many unsolved issues remain.

First, it is important to point out the limitations of our studies to date. All of the Test Cases posed above were well suited to our choice of basis; for our other unpublished tests on objective functions outside these basis sets, the method fails spectacularly. Namely, the Linear Basis is able to recover the Active Subspace, but the introduction of additional basis functions `drowns out' the Active Subspace, and the residuals fail to converge. The notion of what makes for a good basis in this context is a crucial issue which must be solved.

Second, the robustness of the solver must be improved. Occasionally, the solver restart heuristic will fail to find a good initial guess, and the issue will cascade to subsequent stages. A better method is needed. Particularly, a strategy well-adapted to local minima is necessary to overcome our solver issues. A potential extension would be to add a cross-entropy stage to find a good initial guess, then leverage our existing quasi-Newton solver for fast convergence to the optimal point.\cite{Rubinstein2005}

Finally, the dependence on samples of $\vx_i$ and the numer of evaluations of $f$ is of critical importance. Our work here suggests that the cost in $f$ may not be very large, though further studies are absolutely necessary. Most importantly, the case of approximate Active Manifolds has not been addressed here; in this case, the cost in $f$ may scale more strongly. This is a critical area of future work.

% Study what makes a `good' sample of $\vx_i$, how many samples are needed.
% Better restart heuristic

% produces the bibliography section when processed by BibTeX
\bibliography{bibtex_database}
\bibliographystyle{aiaa}

\newpage
%-------------------------------------------------
\section{Appendix} \label{sec:appendix}
%-------------------------------------------------
\subsection{Subspace Distance}
The distance between subspaces measures the distance between the ranges of two different linear subspaces, and is defined\cite{constantine2015}

\begin{equation}
\label{eq:subspace_distance}
\text{Dist}(\mW_1,\mW_2) = \|\mW_1\mW_1^T - \mW_2\mW_2^T\|_2.
\end{equation}

\subsection{Convergence Plots}
Here is the full set of convergence plots for every Test Case study.

\begin{figure}[!ht]
\centering
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/residual_case1}
 \caption{Residual sequence for Test Case 1.}
 \label{fig:residual_case1}
\end{minipage}%
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/objective_case1}
 \caption{Objective sequence for Test Case 1.}
 \label{fig:objective_case1}
\end{minipage}
\end{figure}

\begin{figure}[!ht]
\centering
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/residual_case2}
 \caption{Residual sequence for Test Case 2.}
 \label{fig:residual_case2}
\end{minipage}%
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/objective_case2}
 \caption{Objective sequence for Test Case 2.}
 \label{fig:objective_case2}
\end{minipage}
\end{figure}

\begin{figure}[!ht]
\centering
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/residual_case3}
 \caption{Residual sequence for Test Case 3.}
 \label{fig:residual_case3}
\end{minipage}%
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/objective_case3}
 \caption{Objective sequence for Test Case 3.}
 \label{fig:objective_case3}
\end{minipage}
\end{figure}

\begin{figure}[!ht]
\centering
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/residual_case4}
 \caption{Residual sequence for Test Case 4.}
 \label{fig:residual_case4}
\end{minipage}%
\begin{minipage}{.50\textwidth}
 \centering
 \includegraphics[width=.8\textwidth]{../images/objective_case4}
 \caption{Objective sequence for Test Case 4.}
 \label{fig:objective_case4}
\end{minipage}
\end{figure}

\end{document}

% - Release $Name:  $ -
